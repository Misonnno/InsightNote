import os
import json
import base64
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, Form, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
import httpx 

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

app = FastAPI()

# 2. å…è®¸è·¨åŸŸ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- é…ç½® ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = os.getenv("GEMINI_BASE_URL", "https://api.gptsapi.net/v1") 
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash") # å³ä½¿æ˜¯2.5ä¹Ÿä¼šå…¼å®¹

# --- Prompt ---
STREAM_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯æ•™æˆã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼ˆä¸è¦ä½¿ç”¨ JSONï¼Œä¸è¦è¾“å‡ºå¤šä½™çš„å¯’æš„ï¼‰ï¼š

# é¢˜ç›®
(è¿™é‡Œæå–æˆ–å¤è¿°é¢˜ç›®)

# æ·±åº¦è§£æ
(è¿™é‡Œè¿›è¡Œè¯¦ç»†æ¨å¯¼ï¼Œæ”¯æŒ LaTeXï¼Œä¾‹å¦‚ $E=mc^2$)

# æœ€ç»ˆç­”æ¡ˆ
(è¿™é‡Œå†™æœ€ç»ˆç»“è®º)

# æ ‡ç­¾
(æ ‡ç­¾1, æ ‡ç­¾2, æ ‡ç­¾3)
"""

class Question(BaseModel):
    text: str

# ğŸ› ï¸ å®¢æˆ·ç«¯æ„é€ å™¨ (ä¸ test_api.py ä¿æŒä¸€è‡´)
def get_client():
    http_client = httpx.Client(trust_env=False) # å¼ºåˆ¶ç›´è¿
    return OpenAI(
        api_key=GEMINI_API_KEY, 
        base_url=GEMINI_BASE_URL,
        timeout=120.0, 
        http_client=http_client
    )

# --- çº¯æ–‡æœ¬æé—®æ¥å£ (éæµå¼) ---
@app.post("/ask_ai")
async def ask_ai(question: Question):
    client = get_client()
    print(f"ğŸ¤– æ”¶åˆ°æ–‡æœ¬æé—®ï¼Œæ­£åœ¨æ€è€ƒ (Model: {GEMINI_MODEL})...")
    
    try:
        # âš¡ï¸ stream=False (ç¨³å¦‚è€ç‹—æ¨¡å¼)
        completion = client.chat.completions.create(
            model=GEMINI_MODEL,
            messages=[
                {"role": "system", "content": STREAM_SYSTEM_PROMPT},
                {"role": "user", "content": question.text}
            ],
            stream=False, 
            temperature=0.7 
        )
        # è·å–å®Œæ•´å†…å®¹
        content = completion.choices[0].message.content
        print("âœ… æ€è€ƒå®Œæˆï¼Œæ­£åœ¨è¿”å›æ•°æ®...")
        return Response(content=content, media_type="text/plain")

    except Exception as e:
        error_msg = f"System Error: {str(e)}"
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {error_msg}")
        return Response(content=error_msg, media_type="text/plain")

# --- å›¾ç‰‡åˆ†ææ¥å£ (éæµå¼) ---
@app.post("/analyze_image")
async def analyze_image(text: str = Form(...), image: UploadFile = File(...)):
    print(f"ğŸ“· æ”¶åˆ°å›¾ç‰‡ï¼Œæ­£åœ¨ä¸Šä¼ å¹¶è§£æ (Model: {GEMINI_MODEL})...")
    
    image_content = await image.read()
    base64_image = base64.b64encode(image_content).decode('utf-8')
    media_type = image.content_type or "image/jpeg"

    client = get_client()

    try:
        # âš¡ï¸ stream=False (ç¨³å¦‚è€ç‹—æ¨¡å¼)
        completion = client.chat.completions.create(
            model=GEMINI_MODEL, 
            messages=[
                {"role": "system", "content": STREAM_SYSTEM_PROMPT},
                {
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": text},
                        {
                            "type": "image_url", 
                            "image_url": {
                                "url": f"data:{media_type};base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            stream=False, 
            temperature=0.7
        )
        content = completion.choices[0].message.content
        print("âœ… è§£æå®Œæˆï¼Œæ­£åœ¨è¿”å›æ•°æ®...")
        return Response(content=content, media_type="text/plain")

    except Exception as e:
        error_msg = f"System Error: {str(e)}"
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {error_msg}")
        return Response(content=error_msg, media_type="text/plain")

if __name__ == "__main__":
    import uvicorn
    # æ‰“å°ä¸€ä¸‹å½“å‰çš„é…ç½®ï¼Œæ–¹ä¾¿äºŒæ¬¡ç¡®è®¤
    print(f"ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    print(f"Using Model: {GEMINI_MODEL}")
    uvicorn.run(app, host="0.0.0.0", port=8000)